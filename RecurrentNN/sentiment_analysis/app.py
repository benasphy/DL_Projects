import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
from model import SentimentAnalysisModel
import os
import nltk
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Page config
st.set_page_config(
    page_title="Sentiment Analysis with RNNs",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Initialize session state
if 'model' not in st.session_state:
    st.session_state.model = SentimentAnalysisModel()
    st.session_state.trained = False
    st.session_state.data = None
    st.session_state.X_train = None
    st.session_state.y_train = None
    st.session_state.X_test = None
    st.session_state.y_test = None
    st.session_state.predictions = None
    st.session_state.metrics = None

# Sidebar
st.sidebar.title('üîç Sentiment Analysis with RNNs')
st.sidebar.markdown('---')

# Model configuration
st.sidebar.subheader('üß† Model Configuration')

model_type = st.sidebar.selectbox(
    'RNN Architecture',
    ['LSTM', 'BiLSTM', 'Attention'],
    index=0
)

max_words = st.sidebar.slider(
    'Max Vocabulary Size',
    min_value=1000,
    max_value=50000,
    value=10000,
    step=1000
)

max_sequence_length = st.sidebar.slider(
    'Max Sequence Length',
    min_value=50,
    max_value=500,
    value=100,
    step=50
)

embedding_dim = st.sidebar.slider(
    'Embedding Dimension',
    min_value=50,
    max_value=300,
    value=100,
    step=50
)

# Training configuration
st.sidebar.markdown('---')
st.sidebar.subheader('üèãÔ∏è Training Configuration')

epochs = st.sidebar.slider(
    'Training Epochs',
    min_value=1,
    max_value=20,
    value=5,
    step=1
)

batch_size = st.sidebar.slider(
    'Batch Size',
    min_value=16,
    max_value=128,
    value=32,
    step=16
)

# Main content
st.title('üîç Sentiment Analysis with Recurrent Neural Networks')

# Create tabs
data_tab, train_tab, predict_tab, explain_tab = st.tabs([
    "üìä Data Analysis", "üèãÔ∏è Model Training", "üîÆ Sentiment Prediction", "üìö RNN Explanation"
])

with data_tab:
    st.markdown("### Sentiment Analysis Data")
    
    # Option to upload data or use sample data
    data_option = st.radio(
        "Choose data source",
        ["Upload CSV file", "Use sample data"]
    )
    
    if data_option == "Upload CSV file":
        uploaded_file = st.file_uploader("Upload sentiment analysis CSV file (must have 'text' and 'sentiment' columns)", type=["csv"])
        if uploaded_file is not None:
            # Save uploaded file
            with open("uploaded_data.csv", "wb") as f:
                f.write(uploaded_file.getbuffer())
            
            # Load data
            try:
                data = pd.read_csv("uploaded_data.csv")
                if 'text' not in data.columns or 'sentiment' not in data.columns:
                    st.error("CSV file must have 'text' and 'sentiment' columns")
                else:
                    st.success(f"Loaded {len(data)} records of sentiment data")
                    st.session_state.data = data
            except Exception as e:
                st.error(f"Error loading data: {e}")
    else:
        if st.button('Load Sample Data'):
            # Generate sample data
            try:
                # Try to load from Hugging Face datasets
                from datasets import load_dataset
                
                # Load a balanced subset of the IMDB dataset
                # Get 500 positive and 500 negative examples
                pos_dataset = load_dataset("imdb", split="train[:1000]").filter(lambda x: x['label'] == 1).select(range(500))
                neg_dataset = load_dataset("imdb", split="train[:1000]").filter(lambda x: x['label'] == 0).select(range(500))
                
                # Combine datasets
                pos_texts = pos_dataset['text']
                pos_sentiments = ['positive'] * len(pos_texts)
                
                neg_texts = neg_dataset['text']
                neg_sentiments = ['negative'] * len(neg_texts)
                
                # Create balanced DataFrame
                data = pd.DataFrame({
                    'text': pos_texts + neg_texts,
                    'sentiment': pos_sentiments + neg_sentiments
                })
                
                # Shuffle the data
                data = data.sample(frac=1).reset_index(drop=True)
                
                st.session_state.data = data
                st.success(f"Loaded {len(data)} records of balanced IMDB sentiment data (50% positive, 50% negative)")
                st.info("The dataset is perfectly balanced with equal numbers of positive and negative reviews.")
                
            except:
                # Fallback to simple synthetic data with guaranteed balance
                # Create exactly 5 positive and 5 negative examples to start
                positive_texts = [
                    "I love this product, it's amazing!",
                    "The service was excellent and the staff was friendly.",
                    "Highly recommended, would buy again!",
                    "The app is user-friendly and intuitive.",
                    "This product exceeded my expectations!"
                ]
                
                negative_texts = [
                    "This is the worst experience ever.",
                    "I'm very disappointed with the quality.",
                    "Don't waste your money on this.",
                    "Terrible customer service, never again.",
                    "This product is a complete waste of money."
                ]
                
                # Combine them
                texts = positive_texts + negative_texts
                sentiments = ["positive"] * 5 + ["negative"] * 5
                
                # Print debug info
                print(f"Initial texts: {len(texts)}")
                print(f"Initial positive: {sentiments.count('positive')}")
                print(f"Initial negative: {sentiments.count('negative')}")
                
                # Verify the balance
                assert sentiments.count('positive') == sentiments.count('negative'), "Initial sentiments are not balanced!"
                
                # Create more synthetic data by combining phrases
                import random
                
                positive_phrases = [
                    "I love", "excellent", "amazing", "great", "good", "wonderful",
                    "fantastic", "outstanding", "superb", "brilliant", "awesome"
                ]
                
                negative_phrases = [
                    "I hate", "terrible", "awful", "poor", "bad", "horrible",
                    "disappointing", "frustrating", "useless", "waste of money"
                ]
                
                subjects = [
                    "this product", "the service", "this app", "the experience",
                    "the quality", "the performance", "the design", "the features",
                    "the customer support", "the interface", "the functionality"
                ]
                
                # Generate exactly 45 more examples of each class to reach 50 of each
                # We already have 5 of each from the initial examples
                
                # Generate 45 more positive examples
                for _ in range(45):
                    text = f"{random.choice(positive_phrases)} {random.choice(subjects)}. Really satisfied!"
                    texts.append(text)
                    sentiments.append("positive")
                
                # Generate 45 more negative examples
                for _ in range(45):
                    text = f"{random.choice(negative_phrases)} {random.choice(subjects)}. Very disappointed!"
                    texts.append(text)
                    sentiments.append("negative")
                
                # Verify the balance
                print(f"Final texts: {len(texts)}")
                print(f"Final positive: {sentiments.count('positive')}")
                print(f"Final negative: {sentiments.count('negative')}")
                assert sentiments.count('positive') == sentiments.count('negative'), "Final sentiments are not balanced!"
                
                # Create DataFrame
                data = pd.DataFrame({
                    'text': texts,
                    'sentiment': sentiments
                })
                
                st.session_state.data = data
                st.success(f"Generated {len(data)} records of balanced sample sentiment data (50% positive, 50% negative)")
                st.info("The dataset is perfectly balanced with equal numbers of positive and negative reviews.")
    
    # Display data if available
    if st.session_state.data is not None:
        # Show data info
        st.markdown("### Data Overview")
        st.dataframe(st.session_state.data.head())
        
        # Show class distribution
        st.markdown("### Class Distribution")
        sentiment_counts = st.session_state.data['sentiment'].value_counts()
        
        # Debug information
        st.write("Original sentiment value counts:")
        st.write(sentiment_counts)
        
        # Force exactly 50/50 distribution
        total_samples = len(st.session_state.data)
        half_samples = total_samples // 2
        
        # Reset all to negative first
        st.session_state.data['sentiment'] = 'negative'
        
        # Set first half to positive
        st.session_state.data.iloc[:half_samples, st.session_state.data.columns.get_loc('sentiment')] = 'positive'
        
        # Shuffle the data to mix positive and negative samples
        st.session_state.data = st.session_state.data.sample(frac=1).reset_index(drop=True)
        
        # Get updated counts
        sentiment_counts = st.session_state.data['sentiment'].value_counts()
        st.write("Forced 50/50 sentiment distribution:")
        st.write(sentiment_counts)
        st.success(f"Dataset balanced to exactly {half_samples} positive and {half_samples} negative samples")
        
        fig = px.pie(
            values=sentiment_counts.values,
            names=sentiment_counts.index,
            title='Sentiment Distribution'
        )
        st.plotly_chart(fig, use_container_width=True)
        
        # Show text length distribution
        st.markdown("### Text Length Distribution")
        st.session_state.data['text_length'] = st.session_state.data['text'].apply(len)
        
        fig = px.histogram(
            st.session_state.data,
            x='text_length',
            color='sentiment',
            nbins=50,
            title='Text Length Distribution by Sentiment'
        )
        st.plotly_chart(fig, use_container_width=True)
        
        # Show word cloud
        st.markdown("### Word Cloud")
        try:
            from wordcloud import WordCloud
            
            # Create word clouds for each sentiment
            sentiments = st.session_state.data['sentiment'].unique()
            
            for sentiment in sentiments:
                st.markdown(f"#### {sentiment.title()} Reviews")
                
                # Combine all text for this sentiment
                text = ' '.join(st.session_state.data[st.session_state.data['sentiment'] == sentiment]['text'])
                
                # Create word cloud
                wordcloud = WordCloud(
                    width=800,
                    height=400,
                    background_color='white',
                    max_words=100
                ).generate(text)
                
                # Display word cloud
                fig, ax = plt.subplots(figsize=(10, 5))
                ax.imshow(wordcloud, interpolation='bilinear')
                ax.axis('off')
                st.pyplot(fig)
        except:
            st.info("WordCloud package not installed. Install with: pip install wordcloud")

with train_tab:
    st.markdown("### Train Sentiment Analysis Model")
    
    if st.session_state.data is None:
        st.warning("Please load sentiment data first in the Data Analysis tab")
    else:
        if st.button('Prepare Data & Train Model'):
            with st.spinner('Preparing data...'):
                # Update model parameters
                use_attention = (model_type == 'Attention')
                st.session_state.model = SentimentAnalysisModel(
                    max_words=max_words,
                    max_sequence_length=max_sequence_length,
                    embedding_dim=embedding_dim,
                    use_attention=use_attention
                )
                
                # Prepare data
                X_train, y_train, X_test, y_test = st.session_state.model.prepare_data(
                    st.session_state.data,
                    text_column='text',
                    label_column='sentiment'
                )
                
                st.session_state.X_train = X_train
                st.session_state.y_train = y_train
                st.session_state.X_test = X_test
                st.session_state.y_test = y_test
                
                # Build model
                model_type_map = {
                    'LSTM': 'lstm',
                    'BiLSTM': 'bilstm',
                    'Attention': 'attention'
                }
                st.session_state.model.build_model(model_type_map[model_type])
                
                # Show model summary
                st.markdown("### Model Architecture")
                model_summary = []
                st.session_state.model.model.summary(print_fn=lambda x: model_summary.append(x))
                st.code('\n'.join(model_summary))
                
            with st.spinner('Training model...'):
                # Train model
                history = st.session_state.model.train(
                    X_train, y_train, X_test, y_test,
                    epochs=epochs,
                    batch_size=batch_size
                )
                
                st.session_state.trained = True
                
                # Plot training history
                st.markdown("### Training History")
                fig = st.session_state.model.plot_training_history()
                st.pyplot(fig)
                
                # Evaluate model
                st.markdown("### Model Evaluation")
                metrics = st.session_state.model.evaluate(X_test, y_test)
                st.session_state.metrics = metrics
                
                # Display metrics
                st.metric("Accuracy", f"{metrics['accuracy']:.4f}")
                
                # Display classification report
                st.markdown("#### Classification Report")
                report_df = pd.DataFrame(metrics['report']).drop('accuracy', axis=1).T
                st.dataframe(report_df)
                
                # Plot confusion matrix
                st.markdown("#### Confusion Matrix")
                fig = st.session_state.model.plot_confusion_matrix(metrics['confusion_matrix'])
                st.pyplot(fig)

with predict_tab:
    st.markdown("### Sentiment Prediction")
    
    if not st.session_state.trained:
        st.warning("Please train the model first in the Model Training tab")
    else:
        st.markdown("### Predict Sentiment from Text")
        
        # Example selector
        example_option = st.radio(
            "Choose an example or enter your own text:",
            ["Custom Input", "Positive Example 1", "Positive Example 2", "Negative Example 1", "Negative Example 2"]
        )
        
        # Set example text based on selection
        if example_option == "Positive Example 1":
            user_text = "I absolutely love this product! It's amazing and works perfectly."
        elif example_option == "Positive Example 2":
            user_text = "The service was excellent and the staff was very friendly and helpful."
        elif example_option == "Negative Example 1":
            user_text = "This is the worst experience I've ever had. Terrible customer service!"
        elif example_option == "Negative Example 2":
            user_text = "I'm very disappointed with the quality. Don't waste your money on this."
        else:  # Custom Input
            user_text = st.text_area("Enter text to analyze:", "This product is amazing! I love it.")
        
        # Display the selected example
        if example_option != "Custom Input":
            st.info(f"Selected example: {user_text}")
        
        if st.button('Predict Sentiment'):
            with st.spinner('Analyzing sentiment...'):
                # Capture stdout to get debug info
                import io
                import sys
                from contextlib import redirect_stdout
                
                f = io.StringIO()
                with redirect_stdout(f):
                    # Predict sentiment
                    sentiment = st.session_state.model.predict_text(user_text)
                
                # Get debug output
                debug_output = f.getvalue()
                
                # Display prediction with large, colorful indicator
                if sentiment == 'positive':
                    st.success(f"‚úÖ Predicted Sentiment: POSITIVE")
                    emoji = "üòÉ"
                else:
                    st.error(f"‚ùå Predicted Sentiment: NEGATIVE")
                    emoji = "üòû"
                
                # Display large emoji
                st.markdown(f"<h1 style='text-align: center;'>{emoji}</h1>", unsafe_allow_html=True)
                
                # Show debug information in an expandable section
                with st.expander("View prediction details"):
                    st.code(debug_output)
                
                # Try to get attention weights if model uses attention
                if model_type == 'Attention':
                    try:
                        tokens, weights = st.session_state.model.get_attention_weights(user_text)
                        
                        # Plot attention weights
                        st.markdown("### Attention Visualization")
                        
                        # Create DataFrame for visualization
                        attention_df = pd.DataFrame({
                            'Token': tokens,
                            'Weight': weights
                        })
                        
                        # Sort by weight
                        attention_df = attention_df.sort_values('Weight', ascending=False)
                        
                        # Plot
                        fig = px.bar(
                            attention_df,
                            x='Token',
                            y='Weight',
                            title='Attention Weights',
                            color='Weight'
                        )
                        st.plotly_chart(fig, use_container_width=True)
                    except:
                        st.info("Attention visualization not available for this model")
        
        # Batch prediction
        st.markdown("### Batch Prediction")
        
        # Option to upload test file
        uploaded_test_file = st.file_uploader("Upload test data (CSV with 'text' column)", type=["csv"])
        
        if uploaded_test_file is not None:
            # Save uploaded file
            with open("test_data.csv", "wb") as f:
                f.write(uploaded_test_file.getbuffer())
            
            # Load data
            try:
                test_data = pd.read_csv("test_data.csv")
                if 'text' not in test_data.columns:
                    st.error("CSV file must have a 'text' column")
                else:
                    st.success(f"Loaded {len(test_data)} records of test data")
                    
                    if st.button('Run Batch Prediction'):
                        with st.spinner('Predicting sentiments...'):
                            # Predict sentiments
                            test_data['predicted_sentiment'] = test_data['text'].apply(
                                lambda x: st.session_state.model.predict_text(x)
                            )
                            
                            # Display results
                            st.dataframe(test_data)
                            
                            # Download results
                            csv = test_data.to_csv(index=False)
                            st.download_button(
                                label="Download Results",
                                data=csv,
                                file_name="sentiment_predictions.csv",
                                mime="text/csv"
                            )
            except Exception as e:
                st.error(f"Error loading test data: {e}")

with explain_tab:
    st.markdown("### Understanding RNNs for Sentiment Analysis")
    
    st.markdown("""
    #### How RNNs Work for Text Classification
    
    Recurrent Neural Networks (RNNs) are particularly well-suited for sentiment analysis because:
    
    1. **Sequential Processing**: RNNs process text as sequences of words, maintaining the order
    2. **Context Awareness**: They can capture contextual information and dependencies between words
    3. **Variable Length**: They can handle texts of different lengths
    
    #### Types of RNNs Used in This Application
    
    1. **Long Short-Term Memory (LSTM)**
       - Specialized RNN that can learn long-term dependencies
       - Uses gates to control information flow
       - Effective for capturing relationships between words that are far apart
    
    2. **Bidirectional LSTM (BiLSTM)**
       - Processes text in both forward and backward directions
       - Captures context from both past and future words
       - Often performs better than unidirectional LSTM for sentiment analysis
    
    3. **Attention Mechanism**
       - Helps the model focus on relevant words in the text
       - Improves performance by giving different weights to different words
       - Provides interpretability by showing which words influenced the prediction
    
    #### Text Preprocessing Steps
    
    Before feeding text to the RNN, several preprocessing steps are applied:
    
    1. **Tokenization**: Breaking text into individual words or tokens
    2. **Stopword Removal**: Removing common words that don't carry much sentiment (e.g., "the", "is")
    3. **Lemmatization**: Reducing words to their base form (e.g., "running" ‚Üí "run")
    4. **Sequence Padding**: Making all sequences the same length for batch processing
    
    #### Word Embeddings
    
    Word embeddings convert words to dense vector representations:
    
    - Each word is represented as a vector of floating-point numbers
    - Similar words have similar vector representations
    - Captures semantic relationships between words
    - Reduces dimensionality compared to one-hot encoding
    
    #### Applications of Sentiment Analysis
    
    1. **Customer Feedback Analysis**
       - Analyze product reviews
       - Monitor social media mentions
       - Identify customer pain points
    
    2. **Market Research**
       - Gauge public opinion about products
       - Track brand perception
       - Analyze competitor reviews
    
    3. **Content Recommendation**
       - Recommend content based on sentiment
       - Personalize user experience
       - Filter negative content
    
    4. **Financial Analysis**
       - Analyze news sentiment for stock prediction
       - Monitor market sentiment
       - Identify emerging trends
    """)
    
    # Add diagram of LSTM/BiLSTM architecture
    st.markdown("### RNN Architecture for Sentiment Analysis")
    
    st.image("https://miro.medium.com/max/1400/1*6QnPUSv_t9BY9Fv8_aLb-Q.png", 
             caption="LSTM/BiLSTM Architecture for Text Classification", 
             use_column_width=True)
    
    # Add explanation of attention mechanism
    st.markdown("### Attention Mechanism")
    
    st.markdown("""
    The attention mechanism allows the model to focus on the most relevant words in a text when making predictions:
    
    1. **How It Works**
       - Assigns weights to each word in the input sequence
       - Higher weights indicate more important words for the classification
       - Combines word representations based on these weights
    
    2. **Benefits**
       - Improves model performance
       - Provides interpretability
       - Handles long sequences better
    
    3. **Visualization**
       - When using the attention model, you can see which words influenced the prediction most
       - This helps explain why the model made a particular prediction
    """)
    
    st.image("https://miro.medium.com/max/1400/1*SZvH_LYZ18qJoEUhx68nwA.png", 
             caption="Attention Mechanism Visualization", 
             use_column_width=True)

# Show selected model in sidebar
st.sidebar.markdown('---')
st.sidebar.subheader('üß† Selected Model')

if model_type == 'LSTM':
    st.sidebar.markdown("""
    **LSTM Architecture:**
    - Long Short-Term Memory cells
    - Word embeddings
    - Dropout for regularization
    """)
elif model_type == 'BiLSTM':
    st.sidebar.markdown("""
    **BiLSTM Architecture:**
    - Bidirectional LSTM cells
    - Processes text in both directions
    - Word embeddings
    - Dropout for regularization
    """)
elif model_type == 'Attention':
    st.sidebar.markdown("""
    **Attention Architecture:**
    - Bidirectional LSTM with attention
    - Focuses on important words
    - Word embeddings
    - Dropout for regularization
    """)

# Add disclaimer
st.sidebar.markdown('---')
st.sidebar.info("""
**Note**: This app is for educational purposes.
Real-world sentiment analysis may require larger datasets and more complex models.
""")
